{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport os, time, glob\nimport numpy as np\nimport pandas as pd\nimport librosa as lb\nfrom tqdm import tqdm\nimport seaborn as sns\nfrom scipy.io import loadmat\nfrom scipy.signal.windows import gaussian\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn import metrics\nfrom keras.regularizers import l2\nfrom tensorflow.random import set_seed\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalAveragePooling2D, AvgPool2D  \nfrom keras import Model, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.models import load_model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Dataset B","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/eeg-ad-mci-ctrl-csv/3-EEG_AD_MCI_CTRL_CSV'\nsub_lst_all = sorted(glob.glob(BASE_PATH+'/*/*', recursive=True))\n# sub_lst = sorted(glob.glob(BASE_PATH+'/CTRL/CTRL_*.csv', recursive=True))\n# sub_lst = sorted(glob.glob(BASE_PATH+'/AD/AD_*.csv', recursive=True))\nlen(sub_lst_all)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\nX, y = [], []\nsegment_length = 1024\n\nfor count, sub in tqdm(enumerate(sub_lst_all)):\n    signal = np.array(pd.read_csv(sub, header=None).T)\n    \n    filename = os.path.basename(os.path.normpath(sub))\n    class_name = filename.split(\"_\")[0]\n#     print(len(signal[1]))\n    # Calculate the number of segments\n    num_segments = len(signal[1]) // segment_length\n    \n    # Extract segments and labels\n    start, end = 0, segment_length\n    for i in range(num_segments):\n        start, end = i * segment_length, (i+1) * segment_length\n        segment = signal[:, start:end]\n        \n        X.append(segment)\n        y.append(class_name)\n\nX = np.vstack(X)\ny = np.array(y)\nX = np.reshape(X,(-1,19,1024))\n\nprint('Input:', X.shape)\nprint('Output:', y.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_values, counts = np.unique(y, return_counts=True)\n\n# Print the results\nfor value, count in zip(unique_values, counts):\n    print(f\"{value} occurs {count} times\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LOAD Dataset A","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/eeg-frontotemporal-dementia/3_EEG_Frontotemporal_Dementia_AD_CTRL/derivatives'\nsub_lst = sorted(glob.glob(BASE_PATH+'/*/*', recursive=True))\n\ndf = pd.read_csv(\"/kaggle/input/eeg-frontotemporal-dementia/3_EEG_Frontotemporal_Dementia_AD_CTRL/participants.csv\")\nsub_lst = sorted(glob.glob(BASE_PATH+'/**/sub-*_task-eyesclosed_eeg.set', recursive=True))\nlabels = []\nfor sub in sub_lst:\n    labels.extend(df[df[\"participant_id\"] == sub.split('/')[-1].split('_')[0]]['Group'].to_list())\nlabels    \n\nfor lab in sorted(set(labels)):\n    print('{:s} : {:d}'.format(lab, labels.count(lab)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y = [], []\nsegment_length = 1024\n\nfor count, sub in tqdm(enumerate(sub_lst)):\n    signal = np.array(mne.io.read_raw(sub_lst[count], preload=True).to_data_frame().drop(columns=['time'], axis=1)).T\n\n    class_name = labels[count]\n#     print(len(signal[1]))\n    # Calculate the number of segments\n    num_segments = signal.shape[1] // segment_length\n    \n    # Extract segments and labels\n    start, end = 0, segment_length\n    for i in range(num_segments):\n        start, end = i * segment_length, (i+1) * segment_length\n        segment = signal[:, start:end]\n        \n        X.append(segment)\n#         print(X)\n        y.append(class_name)\n\nX = np.array(X)\ny = np.array(y)\nX = np.reshape(X,(-1,19,1024))\n\nprint('Input:', X.shape)\nprint('Output:', y.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split Dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=0)\n\n# y_train =np.array(y_train)\n# y_test =np.array(y_test)\ny_test_1 = np.array(y_test)\n\n# scalar = StandardScaler()\n# X_train = scalar.fit_transform(X_train.reshape(-1,1)).reshape(-1, 16, 1280)\n# X_test  = scalar.transform(X_test.reshape(-1,1)).reshape(-1,16, 1280)\n\n# X_train,y_train,X_test,y_test =SMOTE_Oversampling(X_train,X_test,y_train, y_test)\ny_train =np.array(y_train)\ny_test =np.array(y_test)\nenc = OneHotEncoder()\ny_train = enc.fit_transform(y_train.reshape(-1,1)).toarray()\ny_test = enc.transform(y_test.reshape(-1,1)).toarray()\n\nprint('X_train: {0}\\nX_test : {1}\\ny_train: {2}\\ny_test : {3}'.format(X_train.shape, X_test.shape, y_train.shape, y_test.shape))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train =  X_train.reshape(-1, X_train.shape[1], X_train.shape[2],1)\nX_test = X_test.reshape(-1, X_test.shape[1], X_test.shape[2],1)\nprint('X_train: {0}\\nX_test : {1}'.format(X_train.shape, X_test.shape))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = X_train[0,:].shape\nprint('Input shape: ',input_shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras.backend as K\ndef spe(y_true, y_pred):\n    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n    possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n    return true_negatives / (possible_negatives + K.epsilon())\n\ndef sen(y_true, y_pred): \n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    return true_positives / (possible_positives + K.epsilon())\n\ndef Pre(y_true, y_pred): \n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    return true_positives / (predicted_positives + K.epsilon())\n\n\ndef f1(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n    return f1_val\n\ndef mcc_metric(y_true, y_pred):\n  predicted = tf.cast(tf.greater(y_pred, 0.5), tf.float32)\n  true_pos = tf.math.count_nonzero(predicted * y_true)\n  true_neg = tf.math.count_nonzero((predicted - 1) * (y_true - 1))\n  false_pos = tf.math.count_nonzero(predicted * (y_true - 1))\n  false_neg = tf.math.count_nonzero((predicted - 1) * y_true)\n  x = tf.cast((true_pos + false_pos) * (true_pos + false_neg) \n      * (true_neg + false_pos) * (true_neg + false_neg), tf.float32)\n  return tf.cast((true_pos * true_neg) - (false_pos * false_neg), tf.float32) / tf.sqrt(x)\n\ndef fpr(y_true, y_pred):\n#     predicted = tf.cast(tf.greater(y_pred, 0.5), tf.float32)\n    false_pos = K.sum(K.round(K.clip(y_true, 0, 1)))\n    true_neg = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n    return (false_pos/(false_pos+true_neg))\n    \ndef fnr(y_true, y_pred):\n#     predicted = tf.cast(tf.greater(y_pred, 0.5), tf.float32)\n    false_neg = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n    true_pos = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    return (false_neg/(false_neg+true_pos))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a Model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = X_train[0,:].shape\nmodel = Sequential([Conv2D(filters =10, kernel_size = (3,3), activation ='relu', input_shape = input_shape),\n                    \n                    MaxPool2D(pool_size=(2,21)), \n                    Dropout(0.25),\n                    Conv2D(filters = 20,kernel_size = (3,3), activation ='relu', input_shape = input_shape,  kernel_regularizer=l2(0.02)),\n                    \n#                     GlobalAveragePooling2D(),\n#                     AvgPool2D(),\n                    Flatten(), \n#                     Dropout(0.5),\n                    Dense(80, activation = \"relu\"),\n#                     Dropout(0.25), \n                    Dense(50, activation = \"relu\"), \n                    Dense(3, activation = \"softmax\")])\nmodel.summary()\nstart = time.time()\nopt = Adam(learning_rate=0.0001)\nmodel.compile(loss= 'binary_crossentropy', optimizer = opt ,metrics = ['accuracy',f1,sen,\n                                                                       spe,mcc_metric,\n                                                                       fpr, fnr])\n                                                                       \nhistory = model.fit(X_train, y_train, batch_size=80, epochs=100, shuffle=True,verbose=1,validation_split=0.2)\n\n# print(\"Total time: {0:3.4f} sec\".format(time.time() - start))\n\n# score = model.evaluate(X_train, y_train, verbose=0)\n# print(\"Training Accuracy: {0:3.4f}\".format(score[1]))\n\nscore =model.evaluate(X_test, y_test, verbose=0)\nprint(\"Testing Accuracy: {0:3.4f}\".format(score[1]))\n# print(\"Testing loss: {0:3.4f}\".format(score[0]))\n\n# preds_test = model.predict(X_test) # label scores \n# preds_train = model.predict(X_train)\n# print(confusion_matrix(y_test, y_pred))\n# print('Train loss: {0:3.4f}'.format(score[0]))\n# print('Train accuracy: {0:3.4f}'.format(score[1])\n# model.save(\"ADNet.h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Performance parameter","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, matthews_corrcoef\npreds_test = model.predict(X_test) # label scores \n# Make predictions\ny_pred = np.argmax(preds_test, axis=1) # predicted classes \ny_test = np.argmax(y_test, axis=1) # true classes\n# Calculate metrics\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred, average='weighted')\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Calculate sensitivity, specificity\nsensitivity = recall_score(y_test, y_pred, average='macro')\nspecificity = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n\n# Matthews correlation coefficient (MCC)\nmcc = matthews_corrcoef(y_test, y_pred)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"F1 Score:\", f1)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"Sensitivity:\", sensitivity)\nprint(\"Specificity:\", specificity)\nprint(\"MCC:\", mcc)\nprint(\"Confusion Matrix:\\n\", conf_matrix)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ROC Curve","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom itertools import cycle\ndef plot_roc_curve(y_test, y_pred):\n  \n  n_classes = len(np.unique(y_test))\n  y_test = label_binarize(y_test, classes=np.arange(n_classes))\n  y_pred = label_binarize(y_pred, classes=np.arange(n_classes))\n\n  # Compute ROC curve and ROC area for each class\n  fpr = dict()\n  tpr = dict()\n  roc_auc = dict()\n  for i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n  \n  # Compute micro-average ROC curve and ROC area\n  fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\n  roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n  # First aggregate all false positive rates\n  all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n  # Then interpolate all ROC curves at this points\n  mean_tpr = np.zeros_like(all_fpr)\n  for i in range(n_classes):\n    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n\n  # Finally average it and compute AUC\n  mean_tpr /= n_classes\n\n  fpr[\"macro\"] = all_fpr\n  tpr[\"macro\"] = mean_tpr\n  roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n  # Plot all ROC curves\n  plt.figure(figsize=(5,5))\n#   plt.figure(dpi=300)\n  lw = 2\n  plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n    label=\"micro-average (AUC = {0:0.2f})\".format(roc_auc[\"micro\"]),\n    color=\"deeppink\", linestyle=\":\", linewidth=4,)\n\n  plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n    label=\"macro-average (AUC = {0:0.2f})\".format(roc_auc[\"macro\"]),\n    color=\"navy\", linestyle=\":\", linewidth=4,)\n  class_name = ['AD', 'NC', 'FTD']\n  colors = cycle([\"aqua\", \"darkorange\", \"darkgreen\", \"yellow\", \"blue\"])\n  for i, color in zip(range(len(class_name)), colors):\n    \n        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n                 \n                \n        label=\" {0} (AUC = {1:0.2f})\".format(class_name[i], roc_auc[i]))\n\n  plt.plot([0, 1], [0, 1], \"k--\", lw=lw)\n  plt.xlim([0.0, 1.0])\n  plt.ylim([0.0, 1.05])\n  plt.xlabel(\"False Positive Rate\", weight='bold',fontsize=12)\n  plt.ylabel(\"True Positive Rate\", weight='bold',fontsize=12)\n  plt.tick_params(labelsize=12)\n#   plt.title(\"Receiver Operating Characteristic (ROC) curve\")\n#   fig.tight_layout(w_pad=2, h_pad=2 )\n  \n  plt.legend()\n  plt.savefig('D2_roc_curv.pdf', dpi=300, bbox_inches=\"tight\")\n\nplot_roc_curve(y_test, y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Accuracy and Loss Cuve","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1,2,figsize=(9,3))\nlabels = ['Traning','Validation']\naxs[0,].plot(history.history['accuracy'],'-r', lw=1.5)\naxs[0].plot(history.history['val_accuracy'], '-k')\naxs[0].set_xlabel('Number of epochs', weight='bold',fontsize=11)\naxs[0].set_ylabel('Accuracy', weight='bold',fontsize=12)\naxs[0].set_ylim([0.5, 1.05])\naxs[0].tick_params(labelsize=11)\naxs[0].legend( labels, loc='lower right',ncol=1)\n\naxs[1].plot(history.history['loss'],'-r', lw=1.5)\naxs[1].plot(history.history['val_loss'], '-k')\naxs[1].set_xlabel('Number of epochs', weight='bold',fontsize=11)\naxs[1].set_ylabel('Loss', weight='bold',fontsize=12)\n# axs[1].set_ylim([0, 0.5])\naxs[1].tick_params(labelsize=11)\naxs[1].legend( labels, loc='upper right',ncol=1)\n\nfig.tight_layout(w_pad=2, h_pad=2 )\nplt.savefig('D2_LUCIDNet_loss.pdf')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## FLIOPS calculation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install keras-flops\n\nimport tensorflow as tf\nfrom tensorflow.python.profiler.model_analyzer import profile\nfrom tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n\ndef get_flops(model):\n  forward_pass = tf.function(model.call, input_signature=[tf.TensorSpec(shape=(1,) + model.input_shape[1:])])\n  graph_info = profile(forward_pass.get_concrete_function().graph, options=ProfileOptionBuilder.float_operation())\n  flops = graph_info.total_float_ops\n  return flops\n\nflops = get_flops(model)\nmacs = flops / 2\nprint(f\"MACs: {macs / 1e+9:,} G\")\nprint(f\"FLOPs: {flops / 1e+9:,} G\")\nflops","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inference Time","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport numpy as np\n\ninput_data = X_test  # Replace with your input shape\n# model = VGG16()\n\nstart_time = time.time()\n_ = model.predict(input_data)\nend_time = time.time()\n\ninference_time = end_time - start_time\nprint(\"Inference Time:\", inference_time)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# T-SNE plot of each layer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Assuming your data is stored in variables X_test and y_test\n# Replace these with your actual data\n# X_test = np.random.random((6811, 19, 1024, 1))\n# y_test = np.random.randint(0, 3, (6811,))\n\n# Number of samples to select from each class\nsamples_per_class = 100\n\n# Initialize an array to store the selected samples\nselected_X = np.zeros((3 * samples_per_class, *X_test.shape[1:]))\nselected_y = np.zeros(3 * samples_per_class, dtype=int)  # Use integer array\n\n# Iterate through each class\nfor class_index in range(3):\n    # Get indices of samples for the current class\n    class_indices = np.where(y_test == class_index)[0]\n\n    # Randomly select 'samples_per_class' samples for the current class\n    selected_indices = np.random.choice(class_indices, samples_per_class, replace=False)\n\n    # Copy the selected samples to the new arrays\n    selected_X[class_index * samples_per_class: (class_index + 1) * samples_per_class] = X_test[selected_indices]\n    selected_y[class_index * samples_per_class: (class_index + 1) * samples_per_class] = y_test[selected_indices]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn import preprocessing\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\n\n# Assuming you have the intermediate representations for each layer\n# Replace these with your actual data\nsuccessive_outputs = [layer.output for layer in model.layers]\n\nvisualization_model = tf.keras.models.Model(inputs=model.input, outputs=successive_outputs[0])\nsuccessive_feature_maps_layer0 = visualization_model.predict(selected_X)\nlayer0_data = np.array(successive_feature_maps_layer0)\n\nvisualization_model = tf.keras.models.Model(inputs=model.input, outputs=successive_outputs[1])\nsuccessive_feature_maps_layer1 = visualization_model.predict(selected_X)\nlayer1_data = np.array(successive_feature_maps_layer1)\n\nvisualization_model = tf.keras.models.Model(inputs=model.input, outputs=successive_outputs[2])\nsuccessive_feature_maps_layer2 = visualization_model.predict(selected_X)\nlayer2_data = np.array(successive_feature_maps_layer2)\n\nvisualization_model = tf.keras.models.Model(inputs=model.input, outputs=successive_outputs[3])\nsuccessive_feature_maps_layer3 = visualization_model.predict(selected_X)\nlayer3_data = np.array(successive_feature_maps_layer3)\n\nsuccessive_outputs = [layer.output for layer in model.layers]\nvisualization_model = tf.keras.models.Model(inputs=model.input, outputs=successive_outputs[4])\nsuccessive_feature_maps_layer4 = visualization_model.predict(selected_X)\nlayer4_data = np.array(successive_feature_maps_layer4)\n\nvisualization_model = tf.keras.models.Model(inputs=model.input, outputs=successive_outputs[5])\nsuccessive_feature_maps_layer5 = visualization_model.predict(selected_X)\nlayer5_data = np.array(successive_feature_maps_layer5)\n\nvisualization_model = tf.keras.models.Model(inputs=model.input, outputs=successive_outputs[6])\nsuccessive_feature_maps_layer6 = visualization_model.predict(selected_X)\nlayer6_data = np.array(successive_feature_maps_layer6)\n\nvisualization_model = tf.keras.models.Model(inputs=model.input, outputs=successive_outputs[7])\nsuccessive_feature_maps_layer7 = visualization_model.predict(selected_X)\nlayer7_data = np.array(successive_feature_maps_layer7)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\ndef plot_and_save_tsne(layer_index, layer_data, labels, markers, save_path):\n    # Define t-SNE model\n    tsne = TSNE(n_components=2, random_state=42)\n\n    # Reshape data before applying t-SNE\n    tsne_data = tsne.fit_transform(layer_data.reshape(layer_data.shape[0], -1))\n\n    plt.figure(figsize=(7, 4))\n       \n    # Plot t-SNE visualization for each class with different colors and markers\n    for class_label in range(3):\n        indices = labels == class_label\n        plt.scatter(tsne_data[indices, 0], tsne_data[indices, 1], label=f'Class {class_label}',color=colors[class_label], alpha=1, marker=markers[class_label])\n        plt.tick_params(labelsize=14)\n        plt.locator_params(axis='x', nbins=5)\n        plt.locator_params(axis='y', nbins=5)\n#     plt.legend()\n#     plt.title(f'T-SNE Visualization - Layer {layer_index}')\n    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n    plt.show()\n\n# Assuming you have markers defined as in your code\nmarkers = ['o', 's', '^']\ncolors = ['red', 'green', 'blue']\n\nlabel_encoder = preprocessing.LabelEncoder()\nlabel = label_encoder.fit_transform(selected_y)\nlabels = np.array(label)\n\n# Call the function for each layer\nplot_and_save_tsne(0, layer0_data, labels, markers, 'AD_FTD_NC_layer0_Tsne_plot.pdf')\nplot_and_save_tsne(1, layer1_data, labels, markers, 'AD_FTD_NC_layer1_Tsne_plot.pdf')\nplot_and_save_tsne(2, layer2_data, labels, markers, 'AD_FTD_NC_layer2_Tsne_plot.pdf')\nplot_and_save_tsne(3, layer3_data, labels, markers, 'AD_FTD_NC_layer3_Tsne_plot.pdf')\nplot_and_save_tsne(4, layer4_data, labels, markers, 'AD_FTD_NC_layer4_Tsne_plot.pdf')\nplot_and_save_tsne(5, layer5_data, labels, markers, 'AD_FTD_NC_layer5_Tsne_plot.pdf')\nplot_and_save_tsne(6, layer6_data, labels, markers, 'AD_FTD_NC_layer6_Tsne_plot.pdf')\nplot_and_save_tsne(7, layer7_data, labels, markers, 'AD_FTD_NC_layer7_Tsne_plot.pdf')","metadata":{},"execution_count":null,"outputs":[]}]}